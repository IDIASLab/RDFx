{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d067c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "from sparkkgml.kg import KG\n",
    "from sparkkgml.motifWalks import MotifWalks\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fcecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession \n",
    "spark = SparkSession.builder.getOrCreate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8782504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KG class with the provided RDF data and optional skip predicates\n",
    "kg_instance = KG(location=\"./AM.ttl\", skip_predicates=[], sparkSession=spark)\n",
    "\n",
    "# Create a new GraphFrame from the knowledge graph\n",
    "graph = kg_instance.createKG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecd4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file containing entity path numbers into a pandas DataFrame\n",
    "df = pd.read_csv(\"pyRDF2Vec/entity_path_numbers.csv\")\n",
    "\n",
    "# Filter entities with path numbers less than 1,000,000 and get the unique list of entities\n",
    "entities = df[df['pathNum'] < 1000000]['entity'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d811830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the number of paths and runtime for each iteration\n",
    "path_num = []\n",
    "run_times = []\n",
    "\n",
    "# Loop over the list of entities, processing subsets incrementally\n",
    "for i in range(len(entities)):\n",
    "    \n",
    "    # Initialize the MotifWalks class with the current subset of entities and the Spark session\n",
    "    motifWalks_instance = MotifWalks(kg_instance, entities=entities[:i], sparkSession=spark)\n",
    "    \n",
    "    # Start the timer to measure the runtime of the motif_walk function\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate motif walks with a depth of 4 for the current subset of entities\n",
    "    paths_df = motifWalks_instance.motif_walk(graph, 4)\n",
    "    \n",
    "    # Append the count of paths generated to the path_num list\n",
    "    path_num.append(paths_df.count())\n",
    "\n",
    "    # Stop the timer and calculate the elapsed time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Append the elapsed time to the run_times list\n",
    "    run_times.append(round(end_time - start_time, 3))\n",
    "    \n",
    "    # Print the current list of path numbers and corresponding runtimes\n",
    "    print('path nums:', path_num)\n",
    "    print('run times:', run_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark32_p38_cpu_v3]",
   "language": "python",
   "name": "conda-env-pyspark32_p38_cpu_v3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
