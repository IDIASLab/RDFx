{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0daf6ce-1e11-476b-a45a-06a50ad9feee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import necessary modules from sparkkgml and pyspark\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from sparkkgml.data_acquisition import DataAcquisition\n",
    "from sparkkgml.feature_engineering import FeatureEngineering\n",
    "from sparkkgml.vectorization import Vectorization\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import size\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.sql.types import StringType\n",
    "from sparkkgml.data_augmentation import spark_dbpedia_lookup_linker\n",
    "from sparkkgml.data_augmentation import spark_specific_relation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdde0989-7d4f-4290-acc9-dec9b308a0a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare endpoint and query\n",
    "endpoint = \"https://sparkkgml.arcc.albany.edu/lmdb\"\n",
    "query =\"\"\"SELECT\n",
    "        ?movie\n",
    "        ?movie_title\n",
    "        ?genre\n",
    "        ?director\n",
    "        ?actor\n",
    "        (<http://www.w3.org/2001/XMLSchema#int>(?movie__down_runtime) as ?runtime)\n",
    "        WHERE { \n",
    "        ?movie <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://data.linkedmdb.org/movie/film> . \n",
    "        ?movie <http://data.linkedmdb.org/movie/genre> ?movie__down_genre . ?movie__down_genre <http://data.linkedmdb.org/movie/film_genre_name> ?genre .\n",
    "       \n",
    "        \n",
    "        OPTIONAL { ?movie <http://purl.org/dc/terms/title> ?movie_title . } \n",
    "        OPTIONAL { ?movie <http://data.linkedmdb.org/movie/runtime> ?movie__down_runtime . } \n",
    "        OPTIONAL { ?movie <http://data.linkedmdb.org/movie/actor> ?movie__down_actor . ?movie__down_actor <http://data.linkedmdb.org/movie/actor_name> ?actor . } \n",
    "        OPTIONAL { ?movie <http://data.linkedmdb.org/movie/director> ?movie__down_director .  ?movie__down_director <http://data.linkedmdb.org/movie/director_name> ?director . } \n",
    "        \n",
    "        FILTER (?genre = 'Spy' || ?genre = 'Superhero' )\n",
    "        #FILTER (?genre = 'Spy' || ?genre = 'Superhero' || ?genre = 'Parody' || ?genre = 'Zombie' )\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5d3c37-184c-4ebf-833e-db6d9336ebdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create an instance of DataAcquisition\n",
    "# set parameters for null values\n",
    "dataAcquisitionObject=DataAcquisition(spark)\n",
    "dataAcquisitionObject.set_amputationMethod('nullReplacement')\n",
    "dataAcquisitionObject.set_nullReplacementMethod('customValue')\n",
    "dataAcquisitionObject.set_customValueVariable(-1)\n",
    "dataAcquisitionObject.set_customStringValueVariable(' ')\n",
    "# Retrieve the data as a Spark DataFrame\n",
    "df = dataAcquisitionObject.getDataFrame(endpoint=endpoint, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ae64ab-f947-401b-9548-64287f692335",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# look for matching DBPedia entity and return its URI as a newly added column\n",
    "df_lookup_linked = spark_dbpedia_lookup_linker(\n",
    "        df, column=\"movie_title\", new_attribute_name=\"new_link\",\n",
    "        query_class=\"Film\", max_hits=1, lookup_api=\"KeywordSearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "064c712a-32e3-4d84-8fe3-a5c848d55742",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create attributes from the URI created above and find specific direct relation, add them as a new feature\n",
    "df_specific_relation = spark_specific_relation_generator(df_lookup_linked, \"new_link\")\n",
    "#df_specific_relation.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75cc1399-0a1e-4d1e-b44b-472e602fb647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select the first 10 columns\n",
    "columns_to_keep = df_specific_relation.columns[:10]\n",
    "# select only the first 10 columns from the DataFrame\n",
    "filtered_df = df_specific_relation.select(*columns_to_keep).drop(\"new_link\")\n",
    "#filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12eb3925-7768-42bf-8c9b-60637526f153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create an instance of FeatureEngineering\n",
    "# call getFeatures function and get features for every column\n",
    "featureEngineeringObject=FeatureEngineering()\n",
    "df2,features=featureEngineeringObject.getFeatures(filtered_df)\n",
    "\n",
    "# apply preprocess to transform genres to have only 1 genre per movie\n",
    "filtered_df = df2.filter(size(df2.genre) <= 1)\n",
    "filtered_df = filtered_df.withColumn('genre', concat_ws(',', filtered_df['genre']))\n",
    "features['genre']['isListOfEntries']=False\n",
    "features['genre']['featureType']='Single_Categorical_String'\n",
    "\n",
    "# create an instance of Vectorization module\n",
    "# call vectorize function and digitaze all the features\n",
    "vectorizationObject=Vectorization()\n",
    "digitized_df=vectorizationObject.vectorize(filtered_df,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d79cf21f-2195-4207-a5e6-ba47410ebfa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "accuracy_list=[]\n",
    "# run the code in a loop and append accuracies for every run \n",
    "for i in range(10):\n",
    "    # assemble features into a vector\n",
    "    selected_features=[i for i in digitized_df.columns if i!='genre' and i!='movie']\n",
    "    assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "    assembled_data = assembler.transform(digitized_df)\n",
    "\n",
    "    # split the data into training and testing sets (70% training, 30% testing)\n",
    "    train_data, test_data = assembled_data.randomSplit([0.7, 0.3])\n",
    "    # create a RandomForestClassifier\n",
    "    rf_classifier = RandomForestClassifier(labelCol=\"genre\", featuresCol=\"features\", numTrees=10)\n",
    "    # Train the model\n",
    "    model = rf_classifier.fit(train_data)\n",
    "\n",
    "    # make predictions on both train and test data\n",
    "    train_predictions = model.transform(train_data)\n",
    "    test_predictions = model.transform(test_data)\n",
    "\n",
    "    # evaluate the model on train data using MulticlassClassificationEvaluator\n",
    "    train_evaluator = MulticlassClassificationEvaluator(labelCol=\"genre\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    train_accuracy = train_evaluator.evaluate(train_predictions)\n",
    "\n",
    "    # evaluate the model on test data using MulticlassClassificationEvaluator\n",
    "    test_evaluator = MulticlassClassificationEvaluator(labelCol=\"genre\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    test_accuracy = test_evaluator.evaluate(test_predictions)\n",
    "    accuracy_list.append(round(test_accuracy, 4))\n",
    "    print(test_accuracy)\n",
    "\n",
    "print(accuracy_list)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sparkkgml-featureSelection+dataAugmentation (top 10 feature selected)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
