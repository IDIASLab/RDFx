{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9063529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This experiment is conducted on Oracle OCI using the Data Flow service to understand the impact \n",
    "# of the 'num_partitions' parameter on the overall runtime of SERE.\n",
    "# The 'num_partitions' parameter in the Word2Vec function controls the level of parallelism \n",
    "# during the computation. By varying this parameter, we aim to observe how it affects the time taken \n",
    "# to generate word embeddings in a distributed computing environment like Spark. \n",
    "# The experiment also involves measuring the time taken to generate motif walks and combining \n",
    "# it with the embedding time to get the total execution time. \n",
    "# The results will provide insights into optimizing 'num_partitions' for large-scale \n",
    "# knowledge graph processing tasks in a distributed setup.\n",
    "\n",
    "# The code initializes a Spark session, loads an RDF dataset to create a knowledge graph,\n",
    "# filters entities based on path numbers, and runs motif walks followed by Word2Vec embeddings. \n",
    "# The execution times for both steps are measured and printed for analysis.\n",
    "\n",
    "# To repeat the experiment in the paper, one needs to increase the num_partitions in the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0202ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ads  # Importing the Oracle Cloud Infrastructure (OCI) Data Science API\n",
    "ads.set_auth(\"resource_principal\")  # Setting the authentication method (options: resource_principal or api_key)\n",
    "\n",
    "# Loading the OCI Data Flow magic commands extension\n",
    "%load_ext dataflow.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dfd22d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json  # Importing the JSON module for handling JSON data\n",
    "\n",
    "# Creating a command dictionary to configure the Data Flow session\n",
    "command = {\n",
    "    \"compartmentId\": \"REMOVED_FOR_SECURITY\",  # OCI Compartment ID\n",
    "    \"displayName\": \"Exec#1\",  # Name for the Data Flow session\n",
    "    \"language\": \"PYTHON\",  # Programming language for the session\n",
    "    \"sparkVersion\": \"3.2.1\",  # Version of Apache Spark to be used\n",
    "    \"driverShape\": \"VM.Standard.E3.Flex\",  # Compute shape for the driver node\n",
    "    \"executorShape\": \"VM.Standard.E3.Flex\",  # Compute shape for the executor nodes\n",
    "    \"driverShapeConfig\": {\"ocpus\": 4, \"memoryInGBs\": 32},  # Configuration for the driver node: 4 OCPUs and 32GB memory\n",
    "    \"executorShapeConfig\": {\"ocpus\": 2, \"memoryInGBs\": 16},  # Configuration for executor nodes: 2 OCPUs and 16GB memory\n",
    "    \"numExecutors\": 1,  # Number of executor nodes\n",
    "    \"type\": \"SESSION\",  # Type of Data Flow execution (SESSION)\n",
    "    \"archiveUri\": \"REMOVED_FOR_SECURITY\"  # URI to the archived job files\n",
    "}\n",
    "\n",
    "# Converting the command dictionary to a JSON string and escaping quotes for shell execution\n",
    "command = f'\\'{json.dumps(command)}\\''\n",
    "# Creating a new Data Flow session with the specified command\n",
    "%create_session -l python -c $command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d889ad2-987c-4c86-aeac-2fa5028dd754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "from sparkkgml.kg import KG\n",
    "from sparkkgml.motifWalks import MotifWalks\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4158bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Initialize the KG class with the provided RDF data and optional skip predicates\n",
    "kg_instance = KG(location=\"/opt/dataflow/python/lib/user/AM/rdf_am-data.ttl\", skip_predicates=[], sparkSession=spark)\n",
    "\n",
    "# Create a new GraphFrame from the knowledge graph\n",
    "graph = kg_instance.createKG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a351e6-cbea-42fe-aa45-5f9e50cc311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Load the CSV file containing entity path numbers into a pandas DataFrame\n",
    "df = pd.read_csv(\"/opt/dataflow/python/lib/user/AM/entity_path_numbers.csv\")\n",
    "# Filter entities with path numbers less than 1,000,000 and get the unique list of entities\n",
    "entities = df[df['pathNum'] < 1000000]['entity'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ac4c0-c511-439c-8aaf-c8b4afc8b4a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Initialize lists to store path counts, walk times, embedding times, and total times\n",
    "path_num = []\n",
    "walk_times = []\n",
    "embedding_times = []\n",
    "total_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298fe1d-b640-45a3-886b-50ce29027849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Initialize the MotifWalks class with the filtered entities and the Spark session\n",
    "motifWalks_instance = MotifWalks(kg_instance, entities=entities, sparkSession=spark)\n",
    "\n",
    "# Measure the time taken to generate motif walks\n",
    "start_time = time.time()\n",
    "paths_df = motifWalks_instance.motif_walk(graph, 4)  # Generate motif walks with depth 4\n",
    "path_num.append(paths_df.count())  # Count the number of generated paths and store it\n",
    "\n",
    "end_time = time.time()\n",
    "walk_time = end_time - start_time  # Calculate the time taken for motif walks\n",
    "walk_times.append(round(walk_time, 3))  # Store the walk time\n",
    "\n",
    "# Measure the time taken to generate Word2Vec embeddings\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# num_partitions should be changed to understand the affect of it for the experiment \n",
    "embeddings = motifWalks_instance.word2Vec_embeddings(\n",
    "    paths_df, vector_size=100, window_size=5, min_count=5, max_iter=5, step_size=0.025, num_partitions=1, seed=42, input_col=\"paths\", output_col=\"vectors\"\n",
    ")  # Generate Word2Vec embeddings\n",
    "\n",
    "end_time = time.time()\n",
    "embedding_time = end_time - start_time  # Calculate the time taken for embeddings\n",
    "embedding_times.append(round(embedding_time, 3))  # Store the embedding time\n",
    "\n",
    "# Calculate the total time for both motif walks and embeddings\n",
    "total_time = round(walk_time + embedding_time, 3)\n",
    "total_times.append(total_time)\n",
    "\n",
    "# Print the results: path counts, walk times, embedding times, and total times\n",
    "print('path nums:', path_num)\n",
    "print('walk times:', walk_times)\n",
    "print('embedding times:', embedding_times)\n",
    "print('total times:', total_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark32_p38_cpu_v3]",
   "language": "python",
   "name": "conda-env-pyspark32_p38_cpu_v3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
