{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is prepared in Databricks \n",
    "# pip install sparkkgml library\n",
    "!pip install sparkkgml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ebc540-b131-4123-8c25-55482d7cac28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import necessary modules from sparkkgml \n",
    "import time\n",
    "from sparkkgml.data_acquisition import DataAcquisition\n",
    "from sparkkgml.feature_engineering import FeatureEngineering\n",
    "from sparkkgml.vectorization import Vectorization\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c0adc7-3d66-49e6-81cd-0e459608cb84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare endpoint and queries for every size of dataset\n",
    "\n",
    "endpoint1 = \"https://sparkkgml.arcc.albany.edu/sampleMovieRDF10\"\n",
    "endpoint2 = \"https://sparkkgml.arcc.albany.edu/sampleMovieRDF100\"\n",
    "endpoint3 = \"https://sparkkgml.arcc.albany.edu/sampleMovieRDF1000\"\n",
    "endpoint4 = \"https://sparkkgml.arcc.albany.edu/sampleMovieRDF10000\"\n",
    "endpoint5 = \"https://sparkkgml.arcc.albany.edu/sampleMovieRDF100000\"\n",
    "\n",
    "query1 =\"\"\"SELECT  ?movie ?producer ?date ?actor\n",
    "        WHERE {\n",
    "        ?actor  <http://sparkkgml.arcc.albany.edu/sampleMovieRDF10/movie_information.org/actedIn> ?movie.\n",
    "        ?movie <http://sparkkgml.arcc.albany.edu/sampleMovieRDF10/movie_information.org/publishedDate> ?date.\n",
    "        ?movie <http://sparkkgml.arcc.albany.edu/sampleMovieRDF10/movie_information.org/producedBy> ?producer.\n",
    "        } \n",
    "        \"\"\"\n",
    "query2 =\"\"\"SELECT  ?movie ?producer ?date ?actor\n",
    "        WHERE {\n",
    "        ?actor  <http://sparkkgml.arcc.albany.edu/sampleMovieRDF100/movie_information.org/actedIn> ?movie.\n",
    "        ?movie <http://sparkkgml.arcc.albany.edu/sampleMovieRDF100/movie_information.org/publishedDate> ?date.\n",
    "        ?movie <http://sparkkgml.arcc.albany.edu/sampleMovieRDF100/movie_information.org/producedBy> ?producer.\n",
    "        } \n",
    "        \"\"\"\n",
    "query3 =\"\"\"SELECT  ?movie ?producer ?date ?actor\n",
    "        WHERE {\n",
    "        ?actor  <http://sparkkgml.arcc.albany.edu/sampleMovieRDF1000/movie_information.org/actedIn> ?movie.\n",
    "        ?movie <http://sparkkgml.arcc.albany.edu/sampleMovieRDF1000/movie_information.org/publishedDate> ?date.\n",
    "        ?movie <http://sparkkgml.arcc.albany.edu/sampleMovieRDF1000/movie_information.org/producedBy> ?producer.\n",
    "        } \n",
    "        \"\"\"\n",
    "query4 =\"\"\"SELECT  ?movie ?producer ?date ?actor\n",
    "        WHERE {\n",
    "        ?actor  <http://sparkkgml.arcc.albany.edu/sampleMovieRDF10000/movie_information.org/actedIn> ?movie.\n",
    "        ?movie <http://sparkkgml.arcc.albany.edu/sampleMovieRDF10000/movie_information.org/publishedDate> ?date.\n",
    "        ?movie <http://sparkkgml.arcc.albany.edu/sampleMovieRDF10000/movie_information.org/producedBy> ?producer.\n",
    "        } \n",
    "        \"\"\"\n",
    "query5 =\"\"\"SELECT  ?movie ?producer ?date ?actor\n",
    "        WHERE {\n",
    "        ?actor  <http://sparkkgml.arcc.albany.edu/sampleMovieRDF100000/movie_information.org/actedIn> ?movie.\n",
    "        ?movie <http://sparkkgml.arcc.albany.edu/sampleMovieRDF100000/movie_information.org/publishedDate> ?date.\n",
    "        ?movie <http://sparkkgml.arcc.albany.edu/sampleMovieRDF100000/movie_information.org/producedBy> ?producer.\n",
    "        } \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "677eee94-b7c7-48e7-aed1-e36e677fbd78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Size 1\n",
    "\n",
    "# run the code in a loop and append runtimes for every run \n",
    "data_acq_times=[]\n",
    "feature_eng_times=[]\n",
    "vectorization_times=[]\n",
    "total_times=[]\n",
    "\n",
    "for i in range(10):\n",
    "    total_time_start = time.time()\n",
    "    # create an instance of DataAcquisition\n",
    "    # set parameters for null values\n",
    "    dataAcquisitionObject=DataAcquisition(spark)\n",
    "    dataAcquisitionObject.set_amputationMethod('nullReplacement')\n",
    "    dataAcquisitionObject.set_nullReplacementMethod('customValue')\n",
    "    dataAcquisitionObject.set_customValueVariable(-1)\n",
    "    dataAcquisitionObject.set_customStringValueVariable(' ')\n",
    "\n",
    "    # retrieve the data as a Spark DataFrame\n",
    "    start_time1 = time.time()\n",
    "    df = dataAcquisitionObject.getDataFrame(endpoint=endpoint1, query=query)\n",
    "    end_time1 = time.time()\n",
    "    data_acq_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    # apply preprocess\n",
    "    df = df.withColumn(\"movie\", regexp_replace('movie','http://sparkkgml.arcc.albany.edu/sampleMovieRDF10/movie_information.org/movie/',''))\n",
    "    df = df.withColumn(\"producer\", regexp_replace('producer','http://sparkkgml.arcc.albany.edu/sampleMovieRDF10/movie_information.org/producer/',''))\n",
    "    df = df.withColumn(\"actor\", regexp_replace('actor','http://sparkkgml.arcc.albany.edu/sampleMovieRDF10/movie_information.org/actor/',''))\n",
    "\n",
    "\n",
    "    # create an instance of FeatureEngineering\n",
    "    # call getFeatures function and get features for every column\n",
    "    featureEngineeringObject=FeatureEngineering()\n",
    "    start_time1 = time.time()\n",
    "    df2,features=featureEngineeringObject.getFeatures(df)\n",
    "    end_time1 = time.time()\n",
    "    feature_eng_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    # create an instance of Vectorization module\n",
    "    # call vectorize function and digitaze all the features\n",
    "    vectorizationObject=Vectorization()\n",
    "    start_time1 = time.time()\n",
    "    digitized_df=vectorizationObject.vectorize(df2,features)\n",
    "    end_time1 = time.time()\n",
    "    vectorization_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    total_time_end = time.time()\n",
    "    total_times.append(round(total_time_end - total_time_start, 2))\n",
    "\n",
    "print('data_acq_times:',data_acq_times)\n",
    "print('feature_eng_times:',feature_eng_times)\n",
    "print('vectorization_times:',vectorization_times)\n",
    "print('total_times:',total_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cecccc3-6a62-4135-83a4-4f49170110fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Size 2\n",
    "\n",
    "# run the code in a loop and append runtimes for every run data_acq_times=[]\n",
    "feature_eng_times=[]\n",
    "vectorization_times=[]\n",
    "total_times=[]\n",
    "\n",
    "for i in range(10):\n",
    "    total_time_start = time.time()\n",
    "    # create an instance of DataAcquisition\n",
    "    # set parameters for null values\n",
    "    dataAcquisitionObject=DataAcquisition(spark)\n",
    "    dataAcquisitionObject.set_amputationMethod('nullReplacement')\n",
    "    dataAcquisitionObject.set_nullReplacementMethod('customValue')\n",
    "    dataAcquisitionObject.set_customValueVariable(-1)\n",
    "    dataAcquisitionObject.set_customStringValueVariable(' ')\n",
    "\n",
    "    #apply preprocess\n",
    "    df = df.withColumn(\"movie\", regexp_replace('movie','http://sparkkgml.arcc.albany.edu/sampleMovieRDF100/movie_information.org/movie/',''))\n",
    "    df = df.withColumn(\"producer\", regexp_replace('producer','http://sparkkgml.arcc.albany.edu/sampleMovieRDF100/movie_information.org/producer/',''))\n",
    "    df = df.withColumn(\"actor\", regexp_replace('actor','http://sparkkgml.arcc.albany.edu/sampleMovieRDF100/movie_information.org/actor/',''))\n",
    "\n",
    "    # retrieve the data as a Spark DataFrame\n",
    "    start_time1 = time.time()\n",
    "    df = dataAcquisitionObject.getDataFrame(endpoint=endpoint2, query=query2)\n",
    "    end_time1 = time.time()\n",
    "    data_acq_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    # create an instance of FeatureEngineering\n",
    "    # call getFeatures function and get features for every column\n",
    "    featureEngineeringObject=FeatureEngineering()\n",
    "    start_time1 = time.time()\n",
    "    df2,features=featureEngineeringObject.getFeatures(df)\n",
    "    end_time1 = time.time()\n",
    "    feature_eng_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    # create an instance of Vectorization module\n",
    "    # call vectorize function and digitaze all the features\n",
    "    vectorizationObject=Vectorization()\n",
    "    start_time1 = time.time()\n",
    "    digitized_df=vectorizationObject.vectorize(df2,features)\n",
    "    end_time1 = time.time()\n",
    "    vectorization_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    total_time_end = time.time()\n",
    "    total_times.append(round(total_time_end - total_time_start, 2))\n",
    "\n",
    "print('data_acq_times:',data_acq_times)\n",
    "print('feature_eng_times:',feature_eng_times)\n",
    "print('vectorization_times:',vectorization_times)\n",
    "print('total_times:',total_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d98714c2-6571-4f11-bca8-08e617a3ebdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Size 3\n",
    "\n",
    "# run the code in a loop and append runtimes for every run \n",
    "data_acq_times=[]\n",
    "feature_eng_times=[]\n",
    "vectorization_times=[]\n",
    "total_times=[]\n",
    "\n",
    "for i in range(10):\n",
    "    total_time_start = time.time()\n",
    "    # create an instance of DataAcquisition\n",
    "    # set parameters for null values\n",
    "    dataAcquisitionObject=DataAcquisition(spark)\n",
    "    dataAcquisitionObject.set_amputationMethod('nullReplacement')\n",
    "    dataAcquisitionObject.set_nullReplacementMethod('customValue')\n",
    "    dataAcquisitionObject.set_customValueVariable(-1)\n",
    "    dataAcquisitionObject.set_customStringValueVariable(' ')\n",
    "\n",
    "    #apply preprocess\n",
    "    df = df.withColumn(\"movie\", regexp_replace('movie','http://sparkkgml.arcc.albany.edu/sampleMovieRDF1000/movie_information.org/movie/',''))\n",
    "    df = df.withColumn(\"producer\", regexp_replace('producer','http://sparkkgml.arcc.albany.edu/sampleMovieRDF1000/movie_information.org/producer/',''))\n",
    "    df = df.withColumn(\"actor\", regexp_replace('actor','http://sparkkgml.arcc.albany.edu/sampleMovieRDF1000/movie_information.org/actor/',''))\n",
    "\n",
    "    # create an instance of FeatureEngineering\n",
    "    # call getFeatures function and get features for every column\n",
    "    featureEngineeringObject=FeatureEngineering()\n",
    "    start_time1 = time.time()\n",
    "    df2,features=featureEngineeringObject.getFeatures(df)\n",
    "    end_time1 = time.time()\n",
    "    feature_eng_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    # create an instance of Vectorization module\n",
    "    # call vectorize function and digitaze all the features\n",
    "    vectorizationObject=Vectorization()\n",
    "    start_time1 = time.time()\n",
    "    digitized_df=vectorizationObject.vectorize(df2,features)\n",
    "    end_time1 = time.time()\n",
    "    vectorization_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    total_time_end = time.time()\n",
    "    total_times.append(round(total_time_end - total_time_start, 2))\n",
    "\n",
    "print('data_acq_times:',data_acq_times)\n",
    "print('feature_eng_times:',feature_eng_times)\n",
    "print('vectorization_times:',vectorization_times)\n",
    "print('total_times:',total_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85dcb830-c051-4d1e-9bdf-b0bc29ba783d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Size 4\n",
    "\n",
    "# run the code in a loop and append runtimes for every run \n",
    "data_acq_times=[]\n",
    "feature_eng_times=[]\n",
    "vectorization_times=[]\n",
    "total_times=[]\n",
    "\n",
    "for i in range(10):\n",
    "    total_time_start = time.time()\n",
    "    # create an instance of DataAcquisition\n",
    "    # set parameters for null values\n",
    "    dataAcquisitionObject=DataAcquisition(spark)\n",
    "    dataAcquisitionObject.set_amputationMethod('nullReplacement')\n",
    "    dataAcquisitionObject.set_nullReplacementMethod('customValue')\n",
    "    dataAcquisitionObject.set_customValueVariable(-1)\n",
    "    dataAcquisitionObject.set_customStringValueVariable(' ')\n",
    "\n",
    "    # retrieve the data as a Spark DataFrame\n",
    "    start_time1 = time.time()\n",
    "    df = dataAcquisitionObject.getDataFrame(endpoint=endpoint4, query=query4)\n",
    "    end_time1 = time.time()\n",
    "    data_acq_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    # apply preprocess\n",
    "    df = df.withColumn(\"movie\", regexp_replace('movie','http://sparkkgml.arcc.albany.edu/sampleMovieRDF10000/movie_information.org/movie/',''))\n",
    "    df = df.withColumn(\"producer\", regexp_replace('producer','http://sparkkgml.arcc.albany.edu/sampleMovieRDF10000/movie_information.org/producer/',''))\n",
    "    df = df.withColumn(\"actor\", regexp_replace('actor','http://sparkkgml.arcc.albany.edu/sampleMovieRDF10000/movie_information.org/actor/',''))\n",
    "\n",
    "\n",
    "    # create an instance of FeatureEngineering\n",
    "    # call getFeatures function and get features for every column\n",
    "    featureEngineeringObject=FeatureEngineering()\n",
    "    start_time1 = time.time()\n",
    "    df2,features=featureEngineeringObject.getFeatures(df)\n",
    "    end_time1 = time.time()\n",
    "    feature_eng_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    # create an instance of Vectorization module\n",
    "    # call vectorize function and digitaze all the features\n",
    "    vectorizationObject=Vectorization()\n",
    "    start_time1 = time.time()\n",
    "    digitized_df=vectorizationObject.vectorize(df2,features)\n",
    "    end_time1 = time.time()\n",
    "    vectorization_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    total_time_end = time.time()\n",
    "    total_times.append(round(total_time_end - total_time_start, 2))\n",
    "\n",
    "print('data_acq_times:',data_acq_times)\n",
    "print('feature_eng_times:',feature_eng_times)\n",
    "print('vectorization_times:',vectorization_times)\n",
    "print('total_times:',total_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c524ef88-d451-4b29-b04e-6cf9040153d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Size 5\n",
    "\n",
    "# run the code in a loop and append runtimes for every run \n",
    "data_acq_times=[]\n",
    "feature_eng_times=[]\n",
    "vectorization_times=[]\n",
    "total_times=[]\n",
    "\n",
    "for i in range(10):\n",
    "    total_time_start = time.time()\n",
    "    # Create an instance of KgQuery\n",
    "    dataAcquisitionObject=DataAcquisition(spark)\n",
    "    dataAcquisitionObject.set_amputationMethod('nullReplacement')\n",
    "    dataAcquisitionObject.set_nullReplacementMethod('customValue')\n",
    "    dataAcquisitionObject.set_customValueVariable(-1)\n",
    "    dataAcquisitionObject.set_customStringValueVariable(' ')\n",
    "\n",
    "    # Retrieve the data as a Spark DataFrame\n",
    "    start_time1 = time.time()\n",
    "    df = dataAcquisitionObject.getDataFrame(endpoint=endpoint5, query=query5)\n",
    "    end_time1 = time.time()\n",
    "    data_acq_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    # apply preprocess\n",
    "    df = df.withColumn(\"movie\", regexp_replace('movie','http://sparkkgml.arcc.albany.edu/sampleMovieRDF100000/movie_information.org/movie/',''))\n",
    "    df = df.withColumn(\"producer\", regexp_replace('producer','http://sparkkgml.arcc.albany.edu/sampleMovieRDF100000/movie_information.org/producer/',''))\n",
    "    df = df.withColumn(\"actor\", regexp_replace('actor','http://sparkkgml.arcc.albany.edu/sampleMovieRDF100000/movie_information.org/actor/',''))\n",
    "\n",
    "    # create an instance of FeatureEngineering\n",
    "    # call getFeatures function and get features for every column\n",
    "    featureEngineeringObject=FeatureEngineering()\n",
    "    start_time1 = time.time()\n",
    "    df2,features=featureEngineeringObject.getFeatures(df)\n",
    "    end_time1 = time.time()\n",
    "    feature_eng_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    # create an instance of Vectorization module\n",
    "    # call vectorize function and digitaze all the features\n",
    "    vectorizationObject=Vectorization()\n",
    "    start_time1 = time.time()\n",
    "    digitized_df=vectorizationObject.vectorize(df2,features)\n",
    "    end_time1 = time.time()\n",
    "    vectorization_times.append(round(end_time1 - start_time1, 2))\n",
    "\n",
    "    total_time_end = time.time()\n",
    "    total_times.append(round(total_time_end - total_time_start, 2))\n",
    "\n",
    "print('data_acq_times:',data_acq_times)\n",
    "print('feature_eng_times:',feature_eng_times)\n",
    "print('vectorization_times:',vectorization_times)\n",
    "print('total_times:',total_times)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sparkkgml-dataSize",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
